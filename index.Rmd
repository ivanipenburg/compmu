---
title:  'Computational Musicology'
author: 'Ilja van Ipenburg'
date:   'February--March 2020'
output: 
    flexdashboard::flex_dashboard:
        storyboard: true
        theme: united
---

```{r setup}
# In order to use these packages, we need to install flexdashboard, plotly, and Cairo.
library(tidyverse)
library(plotly)
library(spotifyr)
library(compmus)
library(tidymodels)
library(protoclust)
library(ggdendro)
library(heatmaply)
source('spotify.R')


joran2016 <- get_playlist_audio_features("spotify", "37i9dQZF1CyJsMsSzzPflw")
joran2017 <- get_playlist_audio_features("spotify", "37i9dQZF1E9TOYaUQk6QFJ")
joran2018 <- get_playlist_audio_features("spotify", "37i9dQZF1EjkhTcRUoUqV5")
joran2019 <- get_playlist_audio_features("spotify", "37i9dQZF1Et8uLwNBVrvuH")
  
ilja2016 <- get_playlist_audio_features("spotify", "37i9dQZF1Cz2hsgWnHLGyO")
ilja2017 <- get_playlist_audio_features("spotify", "37i9dQZF1E9NzC698hTPFX")
ilja2018 <- get_playlist_audio_features("spotify", "37i9dQZF1EjtD9Bk3XI1ge")
ilja2019 <- get_playlist_audio_features("spotify", "37i9dQZF1Etj3WX3nXZqyP")


top2000 <- get_playlist_audio_features("spotify","1DTzz7Nh2rJBnyFbjsH1Mh")

joran <- 
  joran2016 %>% mutate(year="2016") %>%
  bind_rows(joran2017 %>% mutate(year="2017") %>%
  bind_rows(joran2018 %>% mutate(year="2018") %>%
  bind_rows(joran2019 %>% mutate(year="2019"))))

ilja <- 
  ilja2016 %>% mutate(year="2016") %>%
  bind_rows(ilja2017 %>% mutate(year="2017") %>%
  bind_rows(ilja2018 %>% mutate(year="2018") %>%
  bind_rows(ilja2019 %>% mutate(year="2019"))))

data <- 
  joran %>% mutate(person="Joran") %>%
  bind_rows(ilja %>% mutate(person="Ilja"))

mean_joran <- joran %>%
  group_by(year) %>%
  summarise(mean_energy = mean(energy), SD_energy = sd(energy), mean_danceability = mean(danceability), SD_danceability = sd(danceability), mean_valence = mean(valence), SD_valence = sd(valence), mean_acousticness = mean(acousticness), mean_instrumentalness = mean(instrumentalness))
mean_ilja <- ilja %>%
  group_by(year) %>%
  summarise(mean_energy = mean(energy), SD_energy = sd(energy), mean_danceability = mean(danceability), SD_danceability = sd(danceability), mean_valence = mean(valence), SD_valence = sd(valence), mean_acousticness = mean(acousticness), mean_instrumentalness = mean(instrumentalness))


mean <-
  mean_joran %>% mutate(owner="Joran") %>%
  bind_rows(mean_ilja %>% mutate(owner="Ilja"))
```


### Introduction: Brothers leaving the nest, but staying close together
```{r}

ggplot(mean, aes(x = year, linetype = owner, group = owner)) +
  geom_line(aes(y = mean_energy, color="Danceability", alpha=0.9), size = 2) +
  geom_line(aes(y = mean_danceability, color="Energy", alpha=0.9), size = 2) +
  geom_line(aes(y = mean_valence, color="Valence", alpha=0.9), size = 2) +
  geom_line(aes(y = mean_acousticness, color="Acousticness", alpha=0.9), size = 2) +
  geom_line(aes(y = mean_instrumentalness, color="Instrumentalness", alpha=0.9), size = 2) +
  labs(x="Year", y="Mean", colour="Colour", type="Person", title="Feature means over the years") +
  guides(alpha="none") +
  theme_minimal()
```

***

I would say my brother and I have a fairly different taste in music. We both lived with our parents until the end of 2017 approximately, being in close proximity to each other and being exposed to each other's music on a daily basis. Now we study in two different cities and see each other about once every month. In this portfolio I want to look if our proximity, or lack thereof, has had any influence over our respective tastes in music and how our music tastes differ exactly. 

In this first figure the means of five different Spotify features are plotted over the last four years. These numbers were based on our “Top Songs of 20xx” playlists, that are compiled at the end of the year for each Spotify user, and on the features of these playlists that can be extracted with the Spotify API. 

Looking at all the features, you can see that for danceability, energy, and acousticness the lines start to diverge in 2017 and start to converge again in 2019. For the other two features, instumentalness and valence, the means are closest together in 2017 and after that seem to diverge. 

### Our greatest differences: plotting some of our different features

```{r}
ggplot(data, aes(x=danceability, y=valence, color=person, size=instrumentalness)) +
  geom_point(alpha=0.5) +
  facet_wrap(~ year) +
  guides(fill = FALSE) +
  labs(x="Danceability", y="Valence", colour="Person", size="Instrumentalness", title="Valence and danceability over the years") +
  theme_minimal()

```

***

In this plot I explore the most interesting differences in features between us. In the last plot you could see the means of these features plotted over time, but in this plot you can see the features of all the individual songs. 

**Instrumentalness:** The first thing you will most likely notice is the increase of instrumentalness for my brother Joran. This increase could already be seen in the previous plot, but this plot really puts it in perspective. My brother has always listened to a lot of electronic music, mostly drum & bass, but I think in the last few years, his taste in drum & bass has gone from songs which mostly have vocals to very instrumental, bass-heavy songs. For me, I have started listening to more electronic music, but more on the vocal side, so this isn’t represented by the instrumentalness feature.

**Danceability:** This feature can be mostly seen in 2016 and 2017, where my brother’s music is shifted slightly to the right and mine slightly to the left, but when plotted it doesn’t seem quite as significant. For both of us it is quite high and I think that all of our music is quite danceable. For him mostly rock, punk and drum & bass and for me mostly hip hop, indie(pop) and also drum & bass.

**Valence:** As seen in the previous plot, our valence has seemed to switch places.  This can be clearly seen in 2018 and 2019, where the valence for most of my songs is higher than most of my brother’s songs. I don’t have a good explanation for this, but it could be that Spotify doesn’t see the hard drum & bass as very positive.


### The key to a good relationship with your brother: comparing the keys of our music.

```{r}
keys = c("C", "C#", "D", "Eb", "E", "F", "F#", "G", "Ab", "A", "Bb", "B")

keys_ilja <- data %>%
  filter(person=="Ilja") %>%
    ggplot(aes(x=factor(key, labels=keys))) +
      geom_bar(fill="red", alpha=0.6) +
      labs(x="Key", y="Count", title="Ilja") +
      ylim(0, 75) +
      theme_minimal()

keys_joran <- data %>%
  filter(person=="Joran") %>%
    ggplot(aes(x=factor(key, labels=keys))) +
      geom_bar(fill="blue", alpha=0.6) +
      labs(x="Key", y="", title="Joran") +
      ylim(0, 75) +
      theme_minimal()

keys_top2000 <- top2000[0:400,] %>%
  ggplot(aes(x=factor(key, labels=keys))) +
    geom_bar(fill="black", alpha=0.6) +
    labs(x="Key", y="", title="Top 2000 (Selection)") +
    ylim(0, 75) +
    theme_minimal()

require(grid)
require(gridExtra)
grid.arrange(keys_ilja, keys_joran, keys_top2000, ncol=3, top=textGrob("Keys per person",gp=gpar(fontsize=20,font=1)))



```

***

For these graphs I compared the keys of my top songs, my brother's top songs and for a fair comparison also the first 400 songs of the Dutch Top 2000 playlists. I chose this playlist as it seems like a good general playlist that represents the music taste in The Netherlands. 

Surprisingly, the graphs for my brother and I are quite similar, and nothing really stands out except maybe the lack of gaps between G, A and B for my playlist compared to my brothers playlist. To check that this is not just a standard distribution you can look at the top 2000 playlist and see that this has a way different distribution. 

### Gotta go fast, or not: comparing the tempo of our top songs
```{r}
tempo_ilja <- data %>%
  filter(person=="Ilja") %>%
    ggplot(aes(x=tempo)) +
      geom_histogram(fill="red", alpha=0.6) +
      labs(x="Tempo", y="Count", title="Ilja") +
      xlim(40, 200) +
      ylim(0, 50) +
      theme_minimal()

tempo_joran <- data %>%
  filter(person=="Joran") %>%
    ggplot(aes(x=tempo)) +
      geom_histogram(fill="blue", alpha=0.6) +
      labs(x="Tempo", y="", title="Joran") +
      xlim(40, 200) +
      ylim(0, 50) +
      theme_minimal()

tempo_top2000 <- top2000[0:400,] %>%
  ggplot(aes(x=tempo)) +
    geom_histogram(fill="black", alpha=0.6) +
    labs(x="Tempo", y="", title="Top 2000 (Selection)") +
    xlim(40, 200) +
    ylim(0, 50) +
    theme_minimal()

require(grid)
require(gridExtra)
grid.arrange(tempo_ilja, tempo_joran, tempo_top2000, ncol=3, top=textGrob("Tempo per person",gp=gpar(fontsize=20,font=1)))


```

***

When comparing the tempos of our top songs, there is a visible difference. The first thing that you will notice while looking at the graphs is that my brother seems to listen to a lot of songs around the 175 BPM mark. This is very different from the distribution of the top 2000 songs, which can maybe be seen as a sort of average for the Dutch taste. However this peak can easily be explained by my brothers taste for Drum&Bass and other high tempo electronic music, as I also explained earlier. 

The distribution of tempo in my top songs is actually quite similar to the top 2000 distribution and while I do also listen to some electronic music, it is a smaller part of my top songs. I would also say that the electronic music I listen to has a lower tempo than my brothers electronic music on average.

### Comparing two of our top songs (Part 1)

```{r}
devil <- 
  get_tidy_audio_analysis('1UGD3lW3tDmgZfAVDh6w7r') %>% 
  compmus_align(bars, segments) %>% 
  select(bars) %>% unnest(bars) %>% 
  mutate(
    pitches = 
      map(segments, 
          compmus_summarise, pitches, 
          method = 'rms', norm = 'euclidean')) %>% 
  mutate(
    timbre = 
      map(segments, 
          compmus_summarise, timbre, 
          method = 'rms', norm = 'euclidean'))

css_devil_timbre <- compmus_self_similarity(devil, timbre, 'cosine')
  
devil_timbre_plot <- ggplot(css_devil_timbre,
    aes(
      x = xstart + xduration / 2, 
      width = xduration,
      y = ystart + yduration / 2,
      height = yduration,
      fill = d)) + 
  geom_tile() +
  coord_fixed() +
  scale_fill_viridis_c(option = 'B', guide = 'none') +
  theme_classic() +
  labs(x = '', y = '', title="Timbre")

css_devil_chroma <- compmus_self_similarity(devil, pitches, 'cosine')
  
devil_chroma_plot <- ggplot(css_devil_chroma,
    aes(
      x = xstart + xduration / 2, 
      width = xduration,
      y = ystart + yduration / 2,
      height = yduration,
      fill = d)) + 
  geom_tile() +
  coord_fixed() +
  scale_fill_viridis_c(option = 'B', guide = 'none') +
  theme_classic() +
  labs(x = '', y = '', title="Chroma")

require(grid)
require(gridExtra)
grid.arrange(devil_timbre_plot, devil_chroma_plot, ncol=2, top=textGrob("Devil In A New Dress",gp=gpar(fontsize=20,font=1)))


space <- 
  get_tidy_audio_analysis('72Z17vmmeQKAg8bptWvpVG') %>% 
  compmus_align(bars, segments) %>% 
  select(bars) %>% unnest(bars) %>% 
  mutate(
    pitches = 
      map(segments, 
          compmus_summarise, pitches, 
          method = 'rms', norm = 'euclidean')) %>% 
  mutate(
    timbre = 
      map(segments, 
          compmus_summarise, timbre, 
          method = 'rms', norm = 'euclidean'))

css_space_timbre <- compmus_self_similarity(space, timbre, 'cosine')
  
space_timbre_plot <- ggplot(css_space_timbre,
    aes(
      x = xstart + xduration / 2, 
      width = xduration,
      y = ystart + yduration / 2,
      height = yduration,
      fill = d)) + 
  geom_tile() +
  coord_fixed() +
  scale_fill_viridis_c(option = 'B', guide = 'none') +
  theme_classic() +
  labs(x = '', y = '', title="Timbre")

css_space_chroma <- compmus_self_similarity(space, pitches, 'cosine')
  
space_chroma_plot <- ggplot(css_space_chroma,
    aes(
      x = xstart + xduration / 2, 
      width = xduration,
      y = ystart + yduration / 2,
      height = yduration,
      fill = d)) + 
  geom_tile() +
  coord_fixed() +
  scale_fill_viridis_c(option = 'B', guide = 'none') +
  theme_classic() +
  labs(x = '', y = '', title="Chroma")

require(grid)
require(gridExtra)
grid.arrange(space_timbre_plot, space_chroma_plot, ncol=2, top=textGrob("Space Oddity",gp=gpar(fontsize=20,font=1)))
```

***

For these plots we looked at the chroma and timbre of one song for both of us. These songs were selected from our top 10 songs of 2019 and chosen so that they could show us some interesting differences between different parts of the songs.

The song that I chose from my playlist is Devil In A New Dress by Kanye West and Rick Ross. For the first part of the song it has a repeating beat with Kanye West rapping over it, but about 3 minutes into the song a bridge starts with a guitar solo (which is quite unheard of in hip hop music). This can be seen in both the timbre and chromagram around the 200 mark on the x-axis. After this solo Rick Ross starts his verse, which has a similar timbre and chroma to Kanye's part. Lastly, the song ends with another guitar parts, which can be clearly seen in the timbre graph, but less clearly in the chroma graph. This is because in the first solo, it is only the guitar, but at the end, the beat is also still playing through the guitar, so the chromagram doesn't pick it up.

For my brother I chose the song Space Oddity by David Bowie (which to be honest, I wasn't quite familiar with). The song starts with an instrumental, then has its first verse after which is another instrumental. Then there is a second verse, a chorus, a third verse, again the chorus and lastly another intrumental part. Looking closely at the timbre graph, these different parts of the song can be picked out. There are four instumental parts, which can be seen from the four darker 'rectangles' along the diagonal. Between those rectangles are the verses and choruses, of which the choruses have the brightest colours.


### Comparing two of our top songs (Part 2)
```{r}
circshift <- function(v, n) {if (n == 0) v else c(tail(v, n), head(v, -n))}
                                    
    # C     C#    D     Eb    E     F     F#    G     Ab    A     Bb    B 
major_chord <- 
    c(1,    0,    0,    0,    1,    0,    0,    1,    0,    0,    0,    0)
minor_chord <- 
    c(1,    0,    0,    1,    0,    0,    0,    1,    0,    0,    0,    0)
seventh_chord <- 
    c(1,    0,    0,    0,    1,    0,    0,    1,    0,    0,    1,    0)
major_key <- 
    c(6.35, 2.23, 3.48, 2.33, 4.38, 4.09, 2.52, 5.19, 2.39, 3.66, 2.29, 2.88)
minor_key <-
    c(6.33, 2.68, 3.52, 5.38, 2.60, 3.53, 2.54, 4.75, 3.98, 2.69, 3.34, 3.17)
chord_templates <-
    tribble(
        ~name  , ~template,
        'Gb:7'  , circshift(seventh_chord,  6),
        'Gb:maj', circshift(major_chord,    6),
        'Bb:min', circshift(minor_chord,   10),
        'Db:maj', circshift(major_chord,    1),
        'F:min' , circshift(minor_chord,    5),
        'Ab:7'  , circshift(seventh_chord,  8),
        'Ab:maj', circshift(major_chord,    8),
        'C:min' , circshift(minor_chord,    0),
        'Eb:7'  , circshift(seventh_chord,  3),
        'Eb:maj', circshift(major_chord,    3),
        'G:min' , circshift(minor_chord,    7),
        'Bb:7'  , circshift(seventh_chord, 10),
        'Bb:maj', circshift(major_chord,   10),
        'D:min' , circshift(minor_chord,    2),
        'F:7'   , circshift(seventh_chord,  5),
        'F:maj' , circshift(major_chord,    5),
        'A:min' , circshift(minor_chord,    9),
        'C:7'   , circshift(seventh_chord,  0),
        'C:maj' , circshift(major_chord,    0),
        'E:min' , circshift(minor_chord,    4),
        'G:7'   , circshift(seventh_chord,  7),
        'G:maj' , circshift(major_chord,    7),
        'B:min' , circshift(minor_chord,   11),
        'D:7'   , circshift(seventh_chord,  2),
        'D:maj' , circshift(major_chord,    2),
        'F#:min', circshift(minor_chord,    6),
        'A:7'   , circshift(seventh_chord,  9),
        'A:maj' , circshift(major_chord,    9),
        'C#:min', circshift(minor_chord,    1),
        'E:7'   , circshift(seventh_chord,  4),
        'E:maj' , circshift(major_chord,    4),
        'G#:min', circshift(minor_chord,    8),
        'B:7'   , circshift(seventh_chord, 11),
        'B:maj' , circshift(major_chord,   11),
        'D#:min', circshift(minor_chord,    3))
key_templates <-
    tribble(
        ~name    , ~template,
        'Gb:maj', circshift(major_key,  6),
        'Bb:min', circshift(minor_key, 10),
        'Db:maj', circshift(major_key,  1),
        'F:min' , circshift(minor_key,  5),
        'Ab:maj', circshift(major_key,  8),
        'C:min' , circshift(minor_key,  0),
        'Eb:maj', circshift(major_key,  3),
        'G:min' , circshift(minor_key,  7),
        'Bb:maj', circshift(major_key, 10),
        'D:min' , circshift(minor_key,  2),
        'F:maj' , circshift(major_key,  5),
        'A:min' , circshift(minor_key,  9),
        'C:maj' , circshift(major_key,  0),
        'E:min' , circshift(minor_key,  4),
        'G:maj' , circshift(major_key,  7),
        'B:min' , circshift(minor_key, 11),
        'D:maj' , circshift(major_key,  2),
        'F#:min', circshift(minor_key,  6),
        'A:maj' , circshift(major_key,  9),
        'C#:min', circshift(minor_key,  1),
        'E:maj' , circshift(major_key,  4),
        'G#:min', circshift(minor_key,  8),
        'B:maj' , circshift(major_key, 11),
        'D#:min', circshift(minor_key,  3))

devil <- 
    get_tidy_audio_analysis('1UGD3lW3tDmgZfAVDh6w7r') %>% 
    compmus_align(sections, segments) %>% 
    select(sections) %>% unnest(sections) %>% 
    mutate(
        pitches = 
            map(segments, 
                compmus_summarise, pitches, 
                method = 'mean', norm = 'manhattan'))

devil_key_thing <- devil %>% 
    compmus_match_pitch_template(key_templates, 'euclidean', 'manhattan') %>% 
    ggplot(
        aes(x = start + duration / 2, width = duration, y = name, fill = d)) +
    geom_tile() +
    scale_fill_viridis_c(option = 'A', guide = 'none') +
    theme_minimal() +
    labs(x = 'Time (s)', y = '', title="Devil In A New Dress")


space <- 
    get_tidy_audio_analysis('72Z17vmmeQKAg8bptWvpVG') %>% 
    compmus_align(sections, segments) %>% 
    select(sections) %>% unnest(sections) %>% 
    mutate(
        pitches = 
            map(segments, 
                compmus_summarise, pitches, 
                method = 'mean', norm = 'manhattan'))

space_key_thing <- space %>% 
    compmus_match_pitch_template(key_templates, 'euclidean', 'manhattan') %>% 
    ggplot(
        aes(x = start + duration / 2, width = duration, y = name, fill = d)) +
    geom_tile() +
    scale_fill_viridis_c(option = 'A', guide = 'none') +
    theme_minimal() +
    labs(x = 'Time (s)', y = '', title="Space Oddity")

require(grid)
require(gridExtra)
grid.arrange(devil_key_thing, space_key_thing, ncol=2, top=textGrob("Keygrams",gp=gpar(fontsize=20,font=1)))
```

***

In these graphs I look at the keygrams of the same two songs I looked at the chromagrams of, Devil In A New Dress and Space Oddity. 

Devil In A New Dress definitely seems to be in F minor. This is most clearly seen during the verses, and is less clear during the bridge in the middle of the song.

Space Oddity has a less clear key and maybe even seems to switch up for a little bit. The song seems to be in Db major, but during the verses it seems to switch to D minor according to the keygram. I don't think the keys are actually switched during the song, but I can't give a definitive answer as to what key the song is in by only looking at this graph.

### Can an algorithm keep my brother and I apart better than my mom? An exploration of classification algorithms.
```{r}
joran_all <- 
    get_playlist_audio_features('spotify','6AQ2vTw5w1MUrRJU1UPgSq') %>% 
    slice(1:300) %>% 
    add_audio_analysis
ilja_all <- 
    get_playlist_audio_features('spotify','0cZ5aTDYVYK2Fwq0RehG48') %>% 
    slice(1:300) %>% 
    add_audio_analysis

bros <- 
    joran_all %>% mutate(playlist = "Joran") %>% 
    bind_rows(
        ilja_all %>% mutate(playlist = "Ilja")) %>% 
    mutate(playlist = factor(playlist)) %>% 
    mutate(
        segments = 
            map2(segments, key, compmus_c_transpose)) %>% 
    mutate(
        pitches = 
            map(segments, 
                compmus_summarise, pitches, 
                method = 'mean', norm = 'manhattan'),
        timbre =
            map(
                segments,
                compmus_summarise, timbre,
                method = 'mean')) %>% 
    mutate(pitches = map(pitches, compmus_normalise, 'clr')) %>% 
    mutate_at(vars(pitches, timbre), map, bind_rows) %>% 
    unnest(cols = c(pitches, timbre))

bros_class <- 
    recipe(playlist ~
               danceability +
               energy +
               loudness +
               speechiness +
               acousticness +
               instrumentalness +
               liveness +
               valence +
               tempo +
               duration +
               C + `C#|Db` + D + `D#|Eb` +
               E + `F` + `F#|Gb` + G +
               `G#|Ab` + A + `A#|Bb` + B +
               c01 + c02 + c03 + c04 + c05 + c06 +
               c07 + c08 + c09 + c10 + c11 + c12,
           data = bros) %>% 
    step_center(all_predictors()) %>%
    step_scale(all_predictors()) %>%
    # step_range(all_predictors()) %>% 
    prep(bros) %>% 
    juice

bros_cv <- bros_class %>% vfold_cv(10)

bros_knn <- 
    nearest_neighbor(mode = 'classification', neighbors = 1) %>% 
    set_engine('kknn')
predict2_knn <- function(split)
    fit(bros_knn, playlist ~ ., data = analysis(split)) %>% 
    predict(assessment(split), type = 'class') %>%
    bind_cols(assessment(split))

bros_cv %>% 
    mutate(pred = map(splits, predict2_knn)) %>% unnest(pred) %>% 
    conf_mat(truth = playlist, estimate = .pred_class) %>% 
    autoplot(type = 'heatmap', main = 'K-nearest neighbors')

bros_tree <- 
    decision_tree(mode = 'classification') %>%
    set_engine('C5.0')
predict2_tree <- function(split)
    fit(bros_tree, playlist ~ ., data = analysis(split)) %>% 
    predict(assessment(split), type = 'class') %>%
    bind_cols(assessment(split))



bros_cv %>% 
    mutate(pred = map(splits, predict2_tree)) %>% unnest(pred) %>% 
    conf_mat(truth = playlist, estimate = .pred_class) %>% 
    autoplot(type = 'heatmap', main = 'Decision tree')

bros_forest <- 
    rand_forest(mode = 'classification') %>% 
    set_engine('randomForest')
predict2_forest <- function(split)
    fit(bros_forest, playlist ~ ., data = analysis(split)) %>% 
    predict(assessment(split), type = 'class') %>%
    bind_cols(assessment(split))


bros_cv %>% 
    mutate(pred = map(splits, predict2_forest)) %>% unnest(pred) %>% 
    conf_mat(truth = playlist, estimate = .pred_class) %>% 
    autoplot(type = 'heatmap', main = 'Random forest')
```

***
(Just kidding, my mom can keep us apart just fine, only our voices do sound kind of alike) 

An interesting question is if the music taste of me and my brother is different enough for a classifier to see the difference. To see this, we will compare three different classification algorithms that are trained on a dataset of 300 of my top songs and 300 of my brothers top songs. 

**K-nearest neighbors:** The algorithm gave some clear results and in the first graph a clear diagonal line can be seen. For a confusion matrix a solid diagonal line means that the classifier has classified everything correctly, so when you see a diagonal it means it has done a pretty good job.Looking at the numbers, the classifier had an accuracy of 62.8%. The baseline for this prediction is 50%, as that is the accuracy you will get if you pick a person at random. So the classifier is better than picking at random, but definitely not perfect. Let's try to improve this.

**Decision trees:** Using the decision trees method the accuracy increased to 67.0%. The confusion matrix is shown in the second graph. What is interesting to note is that number of correct predictions for my songs increased, but for my brother it slightly decreased. However this is not a big difference and could change each time the classifier is trained.

**Random forests:** Lastly, using the random forests classification method, the accuracy increased to 73.8%. This method definitely seems to be the best for this specific dataset and also provides the best looking confusion matrix as seen in the third graph, with an increase for both playlists.

On the next page we will look at which features were the most important for the accuracy of this random forest classifier.

### Looking at the most important features

```{r}
bros_class %>% 
    fit(bros_forest, playlist ~ ., data = .) %>% 
    pluck('fit') %>% 
    randomForest::varImpPlot()
```

***

As discussed on the previous page, we will now look at the features that were the most important for the random forest classification algorithm. Our top 5 consists of c06, energy, acousticness, c02 and c01. c06, c02 and c01 are three timbre features and these features show us the characteristic quality of sound, independent of pitch or volume. You could see this as the kind of instrumentation. Unfortunately it is hard to say wat these three features are exactly, but it does show us that the top songs of my brother and I do differ a lot in instrumentation. 

```{r, fig.width=3, fig.height=2.5}

ggplot(mean, aes(x = year, linetype = owner, group = owner)) +
  geom_line(aes(y = mean_energy, color="D", alpha=0.9), size = 2) +
  geom_line(aes(y = mean_danceability, color="E", alpha=0.9), size = 2) +
  geom_line(aes(y = mean_acousticness, color="A", alpha=0.9), size = 2) +
  labs(x="Year", y="Mean", colour="Colour", type="Person", title="Feature means over the years") +
  guides(alpha="none") +
  theme_minimal()
```


Energy and acousticness are the other two features that are very important for the classification. Looking back at our first plot, acousticness does seem to differ a lot for the two of us, with the greatest difference in mean being in 2017. However, the mean energy does not seem to differ a lot for us. There is definitely a difference, with my music having a higher energy than my brother's, but just looking at the first plot other features, like danceability, seem to have a much bigger difference. But, looking at the boxplots of these two features, you suddenly see why energy was chosen over danceability and that means can be misleading.

```{r, fig.width=3, fig.height=2.5 }
ggplot(data, aes(x=person, y=energy, fill=person)) +
  geom_boxplot() +
  labs(x="Person", y="Energy", title="Energy per person") +
  theme_minimal()

ggplot(data, aes(x=person, y=danceability, fill=person)) +
  geom_boxplot() +
  labs(x="Person", y="Danceability", title="Danceability per person") +
  theme_minimal() 
```


### Conclusion

This exploration of my brother's music tastes compared to mine showed me a lot about the differences in our music tastes, but also the amount of overlap we have. This was very interesting for me personally, as I also have more insight into our music tastes and our backgrounds, but I hope I have shown some of this in this portfolio. 

By comparing yourself with your siblings, or a partner, or other important people in your life it really makes you think about your taste in music, but also how your tastes have influenced the other's and vice-versa. Over the years, I definitely picked a lot of music up from my brother and looking at his top songs, he did the same. Because we don't live together anymore we don't hear eachother's music that often, but you can still see the influence we have had and will have on the other's tastes.
